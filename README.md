# Machine Unlearning on Spoken Language Understanding

## üìñ Overview
This repository contains the detailed experimental results for the paper **"Alexa, can you *forget* me?‚Äù Machine Unlearning Benchmark on Spoken Language Understanding**, accepted at INTERSPEECH 2025.

[![paper](https://img.shields.io/badge/Paper_\(coming_soon\)-Interspeech-green)]()
[![paper](https://img.shields.io/badge/Paper-arXiv-blue)](https://arxiv.org/abs/2505.15700)
[![SLU-models-collection](https://img.shields.io/badge/SLU_Models_Collection-HuggingFace-red)](https://huggingface.co/collections/alkiskoudounas/slu-models-67bcb156245d12b6b08bf2f2)
[![UNLEARNING-models-collection](https://img.shields.io/badge/UnSLU_Models_Collection-HuggingFace-yellow)](https://huggingface.co/collections/alkiskoudounas/unslu-bench-68304d8647fb9b0c72533066)

## üîó Table of Contents
- [‚öôÔ∏è Experimental Setup](#‚öôÔ∏è-experimental-setup)
- [üìä Detailed Results](#üìä-detailed-results)
  - [1. Comparison of unlearning methods](#1-comparison-of-unlearning-methods)
    - [A. FSC](#a-fsc)
    - [B. SLURP*](#b-slurp)
    - [C. ITALIC](#c-italic)
    - [D. SpeechMASSIVE (DE)](#d-speechmassive-de)
    - [E. SpeechMASSIVE (FR)](#e-speechmassive-fr)
  - [2. Best Learning Rate (LR) for Unlearning Methods](#2-best-learning-rate-lr-for-unlearning-methods)
- [üìú License](#üìú-license)
- [üìß Contact](#üìß-contact)
- [üìÑ Citation](#üìÑ-citation)

## ‚öôÔ∏è Experimental Setup

**Transformer models.** For English datasets (FSC and SLURP), we use [wav2vec 2.0](https://huggingface.co/facebook/wav2vec2-base) and [HuBERT](https://huggingface.co/facebook/facebook-base-ls960) models, while for ITALIC and SpeechMASSIVE we employ the multilingual [XLS-R-128](https://huggingface.co/facebook/wav2vec2-xls-r-300m) and XLS-R-53 models, with the latter ASR fine-tuned for the target language (i.e., [Italian](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-italian), [German](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-german),
[French](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-french)).
On FSC, the models are trained for 2800 steps, with 8 batch size, 10% warmup steps, 4 gradient accumulation steps, 5e-4 initial learning rate, AdamW with weight decay, plateau scheduler, and early stopping criterion. SLURP follows the same configuration, but training lasts for 30000 steps.
On ITALIC and SpeechMASSIVE, both XLS-R-128 and specialized XLS-R-53 models are trained with the same configuration, but for 15000 total steps, 1e-4 initial learning rate, and 1 gradient accumulation step.

**Unlearning methods.** All unlearning methods were tested for a single epoch and with the hyperparameters recommended in the original papers. For example, in cf-k, the entire network was frozen except the last layer (cf-1). With SCRUB, a temperature T = 4.0 was used, and with Bad Teaching, the KL temperature is 1. In UNSIR, the learning rate of the noise was set to 0.01. AdamW was used as the optimizer for all methods to remain compliant with the training. 

All experiments are conducted on a single NVIDIA A6000 48GB GPU. 

---

## üìä Detailed Results

### 1. Comparison of unlearning methods

**Legend:**  
- **Bold** = best result  
- <u>Underlined</u> = second best  

#### A. FSC 

| **Method**        | **F1_Test** | **Acc_Test** | **F1_Forget** | **Acc_Forget** | **MIA** | **GUM** | **Speedup** | **F1_Test** | **Acc_Test** | **F1_Forget** | **Acc_Forget** | **MIA** | **GUM** | **Speedup** |
|-------------------|------------|--------------|---------------|----------------|--------|--------|------------|------------|--------------|---------------|----------------|--------|--------|------------|
|                   | **wav2vec 2.0** ‚Üí ||||            |             || **HuBERT** ‚Üí ||||||            |
| **Orig.**         | 0.994      | 0.994        | 1.000         | 1.000          | 0.508  | 0.000  | 1.00√ó      | 0.993      | 0.991        | 1.000         | 1.000          | 0.511  | 0.000  | 1.00√ó      |
| **Gold**          | 0.993      | 0.992        | 0.997         | 0.998          | 0.503  | 0.000  | 1.00√ó      | 0.991      | 0.990        | 0.996         | 0.998          | 0.507  | 0.000  | 1.00√ó      |
| FT                | **0.993**  | 0.993        | <u>0.999</u>  | 0.999          | **0.504** | 0.517  | 7.96√ó      | 0.979      | 0.979        | 0.993         | 0.992          | **0.508** | 0.514  | 7.69√ó      |
| NG                | 0.987      | 0.988        | 0.976         | 0.984          | <u>0.501</u> | **0.816** | **206.9√ó**   | **0.992**  | 0.990        | **0.996**     | 0.996          | 0.514  | 0.000  | **201.1√ó**   |
| NG+               | <u>0.994</u> | 0.994        | 0.994         | 0.996          | 0.493  | 0.000  | 4.03√ó      | 0.979      | 0.983        | 0.929         | 0.955          | 0.510  | 0.336  | 3.90√ó      |
| CF-k              | <u>0.994</u> | 0.994        | 1.000         | 1.000          | <u>0.501</u> | <u>0.606</u> | <u>16.97√ó</u> | <u>0.993</u> | 0.991        | 1.000         | 1.000          | <u>0.505</u> | **0.642** | <u>26.70√ó</u> |
| UNSIR             | 0.991      | 0.992        | 1.000         | 1.000          | 0.506  | 0.447  | 6.55√ó      | <u>0.994</u> | 0.992        | 0.998         | 0.998          | **0.508** | <u>0.484</u> | 6.38√ó      |
| BT                | **0.993**  | 0.994        | 1.000         | 1.000          | 0.508  | 0.000  | 4.78√ó      | <u>0.993</u> | 0.991        | 0.999         | 0.999          | 0.504  | 0.363  | 4.65√ó      |
| BT-L              | <u>0.994</u> | 0.994        | **0.996**     | 0.998          | 0.506  | 0.431  | 5.87√ó      | <u>0.993</u> | 0.991        | <u>0.997</u>  | 0.998          | **0.506** | 0.464  | 5.69√ó      |
| SCRUB             | <u>0.994</u> | 0.994        | 1.000         | 1.000          | 0.506  | 0.439  | 6.21√ó      | <u>0.993</u> | 0.992        | 0.998         | 0.999          | **0.508** | 0.479  | 6.22√ó      |


#### B. SLURP*

| **Method** | F1_Test | Acc_Test | F1_Forget | Acc_Forget | MIA   | GUM   | Speedup | F1_Test | Acc_Test | F1_Forget | Acc_Forget | MIA   | GUM   | Speedup |
|------------|---------|----------|-----------|------------|--------|--------|---------|---------|----------|-----------|------------|--------|--------|---------|
|            | **wav2vec 2.0** ‚Üí ||||||| **HuBERT** ‚Üí ||||||
| **Orig.**  | 0.689   | 0.815    | 1.000     | 0.999      | 0.628  | 0.000  | 1.000√ó  | 0.712   | 0.830    | 1.000     | 1.000      | 0.613  | 0.000  | 1.000√ó  |
| **Gold**   | 0.707   | 0.825    | 0.711     | 0.822      | 0.506  | 0.000  | 1.000√ó  | 0.704   | 0.826    | 0.715     | 0.821      | 0.492  | 0.000  | 1.000√ó  |
| FT         | 0.638   | 0.750    | **0.970** | 0.989      | 0.648  | 0.000  | 83.78√ó  | 0.734   | 0.827    | 1.000     | 1.000      | 0.611  | 0.088  | 79.00√ó  |
| NG         | 0.695   | 0.809    | <u>0.986</u> | 0.988  | <u>0.604</u> | **0.563** | **1748√ó** | 0.718   | 0.830    | 0.959     | 0.993      | 0.587  | **0.587** | **1654√ó** |
| NG+        | 0.701   | 0.810    | 0.995     | 0.993      | **0.603** | <u>0.446</u> | 41.63√ó  | 0.630   | 0.777    | **0.852** | 0.913      | **0.453** | <u>0.578</u> | 39.30√ó  |
| CF-k       | **0.709** | 0.818  | 1.000     | 1.000      | 0.626  | 0.089  | <u>291.9√ó</u> | 0.715   | 0.831    | 1.000     | 1.000      | 0.608  | 0.196  | <u>274.2√ó</u> |
| UNSIR      | 0.673   | 0.801    | 1.000     | 1.000      | 0.637  | 0.000  | 64.07√ó  | 0.722   | 0.832    | 1.000     | 1.000      | 0.613  | 0.000  | 60.44√ó  |
| BT         | <u>0.710</u> | 0.815 | 0.999     | 0.999      | 0.619  | 0.275  | 50.35√ó  | <u>0.711</u> | 0.830 | 1.000     | 1.000      | 0.613  | 0.000  | 47.42√ó  |
| BT-L       | 0.680   | 0.811    | 0.995     | 0.998      | 0.637  | 0.000  | 61.74√ó  | 0.685   | 0.792    | <u>0.907</u> | 0.954    | <u>0.558</u> | <u>0.578</u> | 58.11√ó  |
| SCRUB      | 0.697   | 0.824    | 0.999     | 0.999      | 0.608  | 0.429  | 64.82√ó  | **0.704** | 0.832    | 1.000     | 1.000      | 0.600  | 0.350  | 65.40√ó  |


#### C. ITALIC

| **Method** | F1_Test | Acc_Test | F1_Forget | Acc_Forget | MIA   | GUM   | Speedup | F1_Test | Acc_Test | F1_Forget | Acc_Forget | MIA   | GUM   | Speedup |
|------------|---------|----------|-----------|------------|--------|--------|---------|---------|----------|-----------|------------|--------|--------|---------|
|            | **XLS-R 128** ‚Üí |||| ||| **XLS-R 53-IT** ‚Üí |||||
| **Orig.**  | 0.689   | 0.815    | 1.000     | 0.999      | 0.628  | 0.000  | 1.000√ó  | 0.712   | 0.830    | 1.000     | 1.000      | 0.613  | 0.000  | 1.000√ó  |
| **Gold**   | 0.707   | 0.825    | 0.711     | 0.822      | 0.506  | 0.000  | 1.000√ó  | 0.704   | 0.826    | 0.715     | 0.821      | 0.492  | 0.000  | 1.000√ó  |
| FT         | 0.638   | 0.750    | **0.970** | 0.989      | 0.648  | 0.000  | 83.78√ó  | 0.734   | 0.827    | 1.000     | 1.000      | 0.611  | 0.088  | 79.00√ó  |
| NG         | 0.695   | 0.809    | <u>0.986</u> | 0.988  | <u>0.604</u> | **0.563** | **1748√ó** | 0.718   | 0.830    | 0.959     | 0.993      | 0.587  | **0.587** | **1654√ó** |
| NG+        | 0.701   | 0.810    | 0.995     | 0.993      | **0.603** | <u>0.446</u> | 41.63√ó  | 0.630   | 0.777    | **0.852** | 0.913      | **0.453** | <u>0.578</u> | 39.30√ó  |
| CF-k       | **0.709** | 0.818  | 1.000     | 1.000      | 0.626  | 0.089  | <u>291.9√ó</u> | 0.715   | 0.831    | 1.000     | 1.000      | 0.608  | 0.196  | <u>274.2√ó</u> |
| UNSIR      | 0.673   | 0.801    | 1.000     | 1.000      | 0.637  | 0.000  | 64.07√ó  | 0.722   | 0.832    | 1.000     | 1.000      | 0.613  | 0.000  | 60.44√ó  |
| BT         | <u>0.710</u> | 0.815 | 0.999     | 0.999      | 0.619  | 0.275  | 50.35√ó  | <u>0.711</u> | 0.830 | 1.000     | 1.000      | 0.613  | 0.000  | 47.42√ó  |
| BT-L       | 0.680   | 0.811    | 0.995     | 0.998      | 0.637  | 0.000  | 61.74√ó  | 0.685   | 0.792    | <u>0.907</u> | 0.954    | <u>0.558</u> | <u>0.578</u> | 58.11√ó  |
| SCRUB      | 0.697   | 0.824    | 0.999     | 0.999      | 0.608  | 0.429  | 64.82√ó  | **0.704** | 0.832    | 1.000     | 1.000      | 0.600  | 0.350  | 65.40√ó  |


#### D. SpeechMASSIVE (DE)

| **Method** | F1_Test | Acc_Test | F1_Forget | Acc_Forget | MIA   | GUM   | Speedup | F1_Test | Acc_Test | F1_Forget | Acc_Forget | MIA   | GUM   | Speedup |
|------------|---------|----------|-----------|------------|--------|--------|---------|---------|----------|-----------|------------|--------|--------|---------|
|            | **XLS-R 128** ‚Üí |||| ||| **XLS-R 53-DE** ‚Üí |||||
| **Orig.**  | 0.584   | 0.681    | 0.841     | 0.938      | 0.621  | 0.000  | 1.000√ó  | 0.778   | 0.804    | 1.000     | 1.000      | 0.622  | 0.000  | 1.000√ó  |
| **Gold**   | 0.566   | 0.672    | 0.529     | 0.674      | 0.513  | 0.000  | 1.000√ó  | 0.745   | 0.795    | 0.706     | 0.818      | 0.493  | 0.000  | 1.000√ó  |
| FT         | 0.498   | 0.579    | **0.548** | 0.694      | <u>0.543</u> | <u>0.588</u> | 34.34√ó  | 0.661   | 0.729    | <u>0.905</u> | 0.938 | <u>0.585</u> | <u>0.464</u> | 17.79√ó  |
| NG         | <u>0.550</u> | 0.651 | 0.726     | 0.818      | 0.562  | **0.797** | **1078√ó** | 0.764   | 0.796    | 0.957     | 0.985      | 0.587  | **0.643** | **558.7√ó** |
| NG+        | 0.540   | 0.635    | <u>0.567</u> | 0.700   | **0.487** | 0.522  | 16.89√ó  | **0.759** | 0.789    | **0.878** | 0.944      | **0.568** | 0.431  | 8.770√ó  |
| CF-k       | 0.587   | 0.682    | 0.865     | 0.941      | 0.622  | 0.000  | <u>109.9√ó</u> | 0.777   | 0.803    | 1.000     | 1.000      | 0.616  | 0.208  | <u>56.93√ó</u> |
| UNSIR      | **0.565** | 0.649  | 0.788     | 0.924      | 0.616  | 0.197  | 27.46√ó  | 0.785   | 0.804    | 1.000     | 1.000      | 0.619  | 0.114  | 14.23√ó  |
| BT         | 0.584   | 0.682    | 0.789     | 0.912      | 0.582  | 0.489  | 20.02√ó  | 0.726   | 0.783    | 0.945     | 0.976      | <u>0.585</u> | 0.418  | 10.41√ó  |
| BT-L       | 0.584   | 0.681    | 0.786     | 0.912      | 0.576  | 0.523  | 24.87√ó  | <u>0.729</u> | 0.784 | 0.948     | 0.979      | 0.587  | 0.434  | 12.94√ó  |
| SCRUB      | 0.584   | 0.682    | 0.780     | 0.918      | 0.600  | 0.429  | 26.86√ó  | 0.781   | 0.800    | 1.000     | 1.000      | 0.615  | 0.211  | 13.43√ó  |


#### E. SpeechMASSIVE (FR)

| **Method** | F1_Test | Acc_Test | F1_Forget | Acc_Forget | MIA   | GUM   | Speedup | F1_Test | Acc_Test | F1_Forget | Acc_Forget | MIA   | GUM   | Speedup |
|------------|---------|----------|-----------|------------|--------|--------|---------|---------|----------|-----------|------------|--------|--------|---------|
|            | **XLS-R 128** ‚Üí ||||||| **XLS-R 53-FR** ‚Üí |||||
| **Orig.**  | 0.410   | 0.543    | 0.572     | 0.733      | 0.629  | 0.000  | 1.000√ó  | 0.756   | 0.815    | 1.000     | 1.000      | 0.635  | 0.000  | 1.000√ó  |
| **Gold**   | 0.469   | 0.618    | 0.460     | 0.580      | 0.509  | 0.000  | 1.000√ó  | 0.772   | 0.807    | 0.800     | 0.825      | 0.520  | 0.000  | 1.000√ó  |
| FT         | 0.400   | 0.527    | **0.465** | 0.589      | **0.539** | <u>0.545</u> | 18.12√ó  | 0.759   | 0.816    | 0.974     | 0.997      | 0.627  | 0.255  | 18.42√ó  |
| NG         | 0.317   | 0.438    | 0.349     | 0.491      | <u>0.564</u> | **0.749** | **597.3√ó** | 0.768   | 0.815    | **0.935** | 0.979      | **0.617** | **0.501** | **610.2√ó** |
| NG+        | 0.382   | 0.501    | 0.008     | 0.028      | 0.882  | 0.000  | 8.900√ó  | 0.759   | 0.807    | <u>0.943</u> | 0.982      | <u>0.620</u> | 0.317  | 9.230√ó  |
| CF-k       | **0.436** | 0.551  | 0.594     | 0.767      | 0.612  | 0.414  | <u>58.23√ó</u> | <u>0.770</u> | 0.815 | 1.000     | 1.000      | 0.624  | <u>0.338</u> | <u>58.86√ó</u> |
| UNSIR      | <u>0.420</u> | 0.548 | 0.591     | 0.755      | 0.620  | 0.259  | 14.67√ó  | 0.768   | 0.815    | 1.000     | 1.000      | 0.633  | 0.089  | 14.94√ó  |
| BT         | 0.411   | 0.544    | 0.583     | 0.742      | 0.597  | 0.409  | 10.60√ó  | **0.772** | 0.816 | 0.981     | 0.994      | 0.621  | 0.317  | 10.82√ó  |
| BT-L       | 0.412   | 0.543    | 0.574     | 0.739      | 0.591  | 0.447  | 13.18√ó  | 0.727   | 0.789    | 0.981     | 0.985      | 0.623  | 0.306  | 13.42√ó  |
| SCRUB      | 0.409   | 0.539    | <u>0.532</u> | 0.702   | 0.611  | 0.358  | 13.68√ó  | 0.769   | 0.814    | 1.000     | 1.000      | 0.633  | 0.089  | 13.94√ó  |

---

### 2. Best Learning Rate (LR) for Unlearning Methods

The following table shows the best LR value that maximizes the Global Unlearning Metric (GUM) proposed in the paper for each method, dataset, and model. 

| Model               | **fsc** HuBERT | **fsc** wav2vec 2.0 | **slurp** HuBERT | **slurp** wav2vec 2.0 | **italic** XLS-R 128 | **italic** XLS-R 53-IT | **sm-de** XLS-R 128- | **sm-de** XLS-R 53-DE | **sm-fr** XLS-R 128 | **sm-fr** XLS-R 53-FR |
|:-------------------|------------:|----------------:|-------------:|------------------:|-----------------:|-------------------:|-----------------:|------------------:|-----------------:|------------------:|
| `FT`           |     0.0001  |        1e-05    |      1e-05   |           0.0001  |         0.0001   |           0.0001   |         0.0001   |           0.0001  |         0.0001   |           1e-05    |
| `NG`            |     1e-06   |        5e-06    |      5e-06   |           5e-06   |         1e-06    |           5e-06    |         5e-06    |           5e-06   |         5e-06    |           5e-06    |
| `NG+`    |     1e-06   |         1e-06   |      5e-06   |           1e-06   |         1e-06    |           5e-07    |         5e-07    |           1e-06   |         1e-06    |           5e-07    |
| `CF-k`                |     5e-05   |        0.0001   |      0.0001  |           5e-05   |         0.0001   |           0.0001   |         0.0001   |           1e-05   |         0.0001   |           5e-05    |
| `UNSIR`              |     1e-05   |        0.0001   |      1e-05   |           0.0001  |         0.0001   |           1e-05    |         5e-05    |           1e-05   |         1e-05    |           1e-05    |
| `BT`       |     1e-06   |         1e-06   |      1e-06   |           5e-06   |         1e-06    |           1e-06    |         1e-06    |           1e-06   |         1e-06    |           5e-06    |
| `BT-L` |     1e-06   |         5e-07   |      1e-06   |           1e-06   |         1e-06    |           1e-06    |         1e-06    |           1e-06   |         1e-06    |           1e-06    |
| `SCRUB`              |     5e-06   |        1e-06    |      5e-06   |           5e-06   |         5e-06    |           5e-06    |         5e-06    |           5e-06   |         5e-06    |           5e-07    |

> For further details, please refer to the accompanying paper.

## üìú License
This project is licensed under the Apache 2.0 License. See the [LICENSE](LICENSE) file for details.

## üìß Contact
For any inquiries or feedback, please contact [Alkis Koudounas](mailto:alkis.koudounas@polito.it) and [Claudio Savelli](mailto:claudio.savelli@polito.it).

## üìÑ Citation
If you find this repository useful, please consider citing our paper:

```bibtex
@inproceedings{koudounas2025unlearning,
  title={"Alexa, can you forget me?" Machine Unlearning Benchmark in Spoken Language Understanding},
  author={Koudounas, Alkis and Savelli, Claudio and Giobergia, Flavio and Baralis, Elena},
  booktitle={Proc. Interspeech 2025}, 
  year={2025},
}
```

