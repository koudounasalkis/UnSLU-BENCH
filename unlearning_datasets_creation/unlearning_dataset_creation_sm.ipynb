{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Â Load Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"de-DE\"\n",
    "dataset = load_dataset(\"FBK-MT/Speech-MASSIVE\", dataset_name)\n",
    "ds_train = dataset[\"train\"]\n",
    "ds_validation = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.DataFrame(ds_train)\n",
    "df_validation = pd.DataFrame(ds_validation)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_ids = df_train[\"speaker_id\"].unique()\n",
    "\n",
    "# remove from val speakers that are in train\n",
    "print(len(df_validation))\n",
    "df_validation = df_validation[~df_validation[\"speaker_id\"].isin(speaker_ids)]\n",
    "print(len(df_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.intent_idx.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakerids = df_train['speaker_id'].value_counts()\n",
    "\n",
    "len(speakerids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakerids.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import set_seed\n",
    "import random\n",
    "\n",
    "def get_forget_retain_split(df_train, min_samples_forget=100, ratio=0.025, seed=42, speaker_col='speakerId'):\n",
    "\n",
    "    speakerids = df_train[speaker_col].value_counts()\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # sample speakers that have at least 200 samples until 2.5% of the total dataset samples are reached\n",
    "    speakers = speakerids[speakerids>min_samples_forget].index.tolist()\n",
    "    total_samples = 0 \n",
    "    speakers_to_sample = []\n",
    "    while total_samples < len(df_train)*ratio:\n",
    "        speaker = random.choice(speakers)\n",
    "        speakers_to_sample.append(speaker)\n",
    "        total_samples += speakerids[speaker]\n",
    "\n",
    "    df_forget = df_train[df_train[speaker_col].isin(speakers_to_sample)]\n",
    "    df_retain = df_train[~df_train[speaker_col].isin(speakers_to_sample)]\n",
    "    return df_forget, df_retain\n",
    "\n",
    "speakerl_col = 'speaker_id'\n",
    "df_forget, df_retain = get_forget_retain_split(df_train, speaker_col=speakerl_col)\n",
    "\n",
    "assert len(df_forget) + len(df_retain) == len(df_train)\n",
    "assert len(set(df_forget[speakerl_col]).intersection(set(df_retain[speakerl_col]))) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_forget) / len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forget.intent_idx.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retain.intent_idx.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# save the indexes in a txt file of the forget samples and the retain one \n",
    "forget_indexes = df_forget.index.tolist()\n",
    "retain_indexes = df_retain.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# save the indexes in a txt file of the forget samples and the retain one \n",
    "forget_indexes = df_forget.index.tolist()\n",
    "retain_indexes = df_retain.index.tolist()\n",
    "\n",
    "os.makedirs(dataset_name, exist_ok=True)\n",
    "\n",
    "with open(f'{dataset_name}/forget_indexes.txt', 'w') as f:\n",
    "    for item in forget_indexes:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(f'{dataset_name}/retain_indexes.txt', 'w') as f:\n",
    "    for item in retain_indexes:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forget_retain_datasets(ds_train, data_path):\n",
    "    with open(data_path + 'forget_indexes.txt') as f:\n",
    "        forget_indexes = f.readlines()\n",
    "    forget_indexes = [int(x.strip()) for x in forget_indexes]\n",
    "\n",
    "    with open(data_path + 'retain_indexes.txt') as f:\n",
    "        retain_indexes = f.readlines()\n",
    "    retain_indexes = [int(x.strip()) for x in retain_indexes]\n",
    "\n",
    "    ds_forget = ds_train.select(forget_indexes)\n",
    "    ds_retain = ds_train.select(retain_indexes)\n",
    "\n",
    "    return ds_forget, ds_retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_forget, ds_retain = get_forget_retain_datasets(ds_train, dataset_name + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_forget, ds_retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "index_val = df_validation.index.tolist()\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(index_val)\n",
    "val_size = len(index_val) // 2\n",
    "index_val_new = index_val[:val_size]\n",
    "index_test_new = index_val[val_size:]\n",
    "\n",
    "with open(f'{dataset_name}/val_indexes.txt', 'w') as f:\n",
    "    for item in index_val_new:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(f'{dataset_name}/test_indexes.txt', 'w') as f:\n",
    "    for item in index_test_new:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
