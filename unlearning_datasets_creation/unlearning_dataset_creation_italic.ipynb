{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "##Â Load Dataset\n",
    "dataset = load_dataset(\"RiTA-nlp/ITALIC\", \"hard_speaker\")\n",
    "ds_train = dataset[\"train\"]\n",
    "ds_validation = dataset[\"validation\"]\n",
    "\n",
    "## Mapping intents to labels\n",
    "intents = set(ds_train['intent'])\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(intents):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "num_labels = len(id2label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.DataFrame(ds_train)\n",
    "df_validation = pd.DataFrame(ds_validation)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_ids = df_train[\"speaker_id\"].unique()\n",
    "\n",
    "# remove from val speakers that are in train\n",
    "print(len(df_validation))\n",
    "df_validation = df_validation[~df_validation[\"speaker_id\"].isin(speaker_ids)]\n",
    "print(len(df_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.intent.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakerids = df_train['speaker_id'].value_counts()\n",
    "\n",
    "len(speakerids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakerids.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import set_seed\n",
    "import random\n",
    "\n",
    "def get_forget_retain_split(df_train, min_samples_forget=100, ratio=0.025, seed=42, speaker_col='speakerId'):\n",
    "\n",
    "    speakerids = df_train[speaker_col].value_counts()\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # sample speakers that have at least 200 samples until 2.5% of the total dataset samples are reached\n",
    "    speakers = speakerids[speakerids>min_samples_forget].index.tolist()\n",
    "    total_samples = 0 \n",
    "    speakers_to_sample = []\n",
    "    while total_samples < len(df_train)*ratio:\n",
    "        speaker = random.choice(speakers)\n",
    "        speakers_to_sample.append(speaker)\n",
    "        total_samples += speakerids[speaker]\n",
    "\n",
    "    df_forget = df_train[df_train[speaker_col].isin(speakers_to_sample)]\n",
    "    df_retain = df_train[~df_train[speaker_col].isin(speakers_to_sample)]\n",
    "    return df_forget, df_retain\n",
    "\n",
    "speakerl_col = 'speaker_id'\n",
    "df_forget, df_retain = get_forget_retain_split(df_train, speaker_col=speakerl_col)\n",
    "\n",
    "assert len(df_forget) + len(df_retain) == len(df_train)\n",
    "assert len(set(df_forget[speakerl_col]).intersection(set(df_retain[speakerl_col]))) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_forget) / len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forget.intent.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retain.intent.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the indexes in a txt file of the forget samples and the retain one \n",
    "forget_indexes = df_forget.index.tolist()\n",
    "retain_indexes = df_retain.index.tolist()\n",
    "\n",
    "with open('forget_indexes.txt', 'w') as f:\n",
    "    for item in forget_indexes:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open('retain_indexes.txt', 'w') as f:\n",
    "    for item in retain_indexes:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forget_retain_datasets(ds_train, data_path):\n",
    "    with open(data_path + 'forget_indexes.txt') as f:\n",
    "        forget_indexes = f.readlines()\n",
    "    forget_indexes = [int(x.strip()) for x in forget_indexes]\n",
    "\n",
    "    with open(data_path + 'retain_indexes.txt') as f:\n",
    "        retain_indexes = f.readlines()\n",
    "    retain_indexes = [int(x.strip()) for x in retain_indexes]\n",
    "\n",
    "    ds_forget = ds_train.select(forget_indexes)\n",
    "    ds_retain = ds_train.select(retain_indexes)\n",
    "\n",
    "    return ds_forget, ds_retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in half validation and test\n",
    "\n",
    "len_ds_validation = len(ds_validation)\n",
    "\n",
    "ds_validation_half = ds_validation.shard(num_shards=2, index=0)\n",
    "\n",
    "ds_test = ds_validation.shard(num_shards=2, index=1)\n",
    "\n",
    "len(ds_validation_half), len(ds_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
